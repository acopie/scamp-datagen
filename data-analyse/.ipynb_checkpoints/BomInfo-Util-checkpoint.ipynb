{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d628a3cd-e84e-4ddc-b6c4-48509c306aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bom Information util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25489a7-1f65-428f-b49d-33d262199ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev, quantiles\n",
    "import networkx as nx\n",
    "import pandas as pd  # data manipulation and analysis\n",
    "import numpy as np  # working with arrays and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef515f93-5a32-4812-9e0a-f95a456a92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path constants\n",
    "DATASET_ROOT_PATH = \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04b9cde-7755-4efd-a8f8-1f74f9d479e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_machine(current_node, product_machine_dictionary={}, metainfo=None):\n",
    "    if metainfo is not None:\n",
    "        for el in metainfo['prod_machines']:\n",
    "            product_machine_dictionary[el['productid']] = el['machines']\n",
    "        return\n",
    "\n",
    "    operation_id = current_node['operationid']\n",
    "    product_id = current_node['productid']\n",
    "    \n",
    "    if 'machines' in current_node:\n",
    "            product_machine_dictionary[product_id] = current_node['machines']\n",
    "    \n",
    "    if 'children' not in current_node:\n",
    "        return\n",
    "    \n",
    "    nodes = current_node['children']\n",
    "    \n",
    "    if len(nodes) == 0:\n",
    "        return\n",
    "  \n",
    "    for node in nodes:\n",
    "        get_product_machine(node, product_machine_dictionary, metainfo)\n",
    "\n",
    "\n",
    "def build_graph_list(data_set_path, filter, build_graph_function,  data_set_filter=[], type_digraph=False):\n",
    "    \"\"\"\n",
    "    Creates a nxGraph list for the test instances of a data set\n",
    "    :param data_set_path - directory for data set\n",
    "    :param filter - used to select only instances files\n",
    "    :param data_set_filter - a list that contains a subset of instances from the data set instances\n",
    "    :param type_digraph - useful in case when the operation graph is visualized in a tree structure\n",
    "    \"\"\"\n",
    "    test_instances = sorted(glob.glob(f'{data_set_path}/{filter}'))\n",
    "    metainfo_path = f'{data_set_path}/metainfo.json'\n",
    "\n",
    "    if len(data_set_filter)!=0:\n",
    "         aux_test_instances = []\n",
    "         for instance in test_instances:\n",
    "             for inst in data_set_filter:\n",
    "                 if inst in instance:\n",
    "                     aux_test_instances.append(instance)\n",
    "         test_instances = aux_test_instances\n",
    "         print(aux_test_instances)\n",
    "        \n",
    "    metainfo = None\n",
    "    if os.path.isfile(metainfo_path):\n",
    "        meta_file = open(metainfo_path)\n",
    "        metainfo = json.load(meta_file)\n",
    "        \n",
    "    graph_list = []\n",
    "    for instance in test_instances:\n",
    "        in_file = open(instance)\n",
    "        root_node = json.load(in_file)\n",
    "        if type_digraph:\n",
    "            G = nx.DiGraph(name=Path(instance).stem)\n",
    "        else:\n",
    "            G = nx.MultiGraph(name=Path(instance).stem)\n",
    "        product_machine_dictionary  = {}\n",
    "        get_product_machine(root_node, product_machine_dictionary, metainfo)\n",
    "        build_graph_function(root_node, G, product_machine_dictionary)\n",
    "        graph_list.append(G)\n",
    "\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c07dda4-cb96-4e50-a853-b451b12985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(build_graph_function, type_digraph=False, edge_info=None):\n",
    "\n",
    "    _2asp_graphs = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/2ASP/', filter=\"*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    \n",
    "    dyuthi_graphs = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/dyuthi/', filter=\"*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "\n",
    "    fjssp_graph = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/FJSSP-Hurink-vdata/', filter=\"*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    # fjssp_graph.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/FJSSP/set1/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "    # fjssp_graph.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/FJSSP/set2/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "\n",
    "    asp_deep_graphs = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/ASP-DEEP/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    \n",
    "    asp_wide_graphs = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/ASP-WIDE/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    \n",
    "    asp_mixed_graphs = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set1/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    # asp_mixed_graphs.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set2/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "    # asp_mixed_graphs.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set3/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "    # asp_mixed_graphs.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set4/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "    # asp_mixed_graphs.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set5/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "    # asp_mixed_graphs.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set6/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "    # asp_mixed_graphs.extend(build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/mixed_boms/set7/', filter=\"*No*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph))\n",
    "\n",
    "    fajp_dafjs_graph = None#build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/dafjs/', filter=\"*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    \n",
    "    fajp_yfjs_graph = build_graph_list(data_set_path=f'{DATASET_ROOT_PATH}/yfjs/', filter=\"*.json\", build_graph_function=build_graph_function, type_digraph=type_digraph)\n",
    "    \n",
    "    return _2asp_graphs, dyuthi_graphs, fjssp_graph, asp_deep_graphs, asp_wide_graphs, asp_mixed_graphs, fajp_dafjs_graph, fajp_yfjs_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0280226b-0278-4c8a-b821-f96c2b876863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_corelations(df):\n",
    "    df = df.drop(['id'], axis=1)\n",
    "    df.describe()\n",
    "\n",
    "    correlation = df.corr()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\n",
    "\n",
    "    plt.title('Correlation between different features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf86950f-6611-4dba-b1a5-6d818bd10c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_with_similar_structure(graphs):\n",
    "    \"\"\"\n",
    "    Identify if in a data set exists graphs with similar structure (graph A is a permuttaion of graph B)\n",
    "    \"\"\"\n",
    "    print(\"Graphs with similar structures\")\n",
    "    similar = [False]*len(graphs)\n",
    "    for i in range(len(graphs)):\n",
    "        if not similar[i]:\n",
    "            found = False\n",
    "            for j in range(i+1, len(graphs)):\n",
    "                if  nx.is_isomorphic(graphs[i], graphs[j]):\n",
    "                    if not found:\n",
    "                         print(\"\\n\", graphs[i].graph['name'], end=\", \")\n",
    "                    found = True\n",
    "                    print(graphs[j].graph['name'], end=\", \")\n",
    "                    similar[j] = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7812b6-9050-4245-becf-5b9eeb7e556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_nodes(graphs):\n",
    "    for g in graphs:\n",
    "        print(g.graph['name'],g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "532fb285-eb47-47c8-a406-a8c5b6d734fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/oliviaguest/gini?tab=readme-ov-file\n",
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq:\n",
    "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
    "    # from:\n",
    "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = np.array(array).flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array = array + 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18d75ee-205b-4972-b03b-5643ec65b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(data, dictionary, data_label, filters=['min', 'max', 'mean', 'stdev', 'q25', 'q50', 'q75', 'gini']):\n",
    "    \"\"\"\n",
    "    get statistic related to data set\n",
    "    :param data - the data series\n",
    "    :param data_label - the label of the data series \n",
    "    :param dictionary -  a dictionary that contains the statistic information \n",
    "    :return: min value; max value; mean value; standard deviation, 3 quantiles\n",
    "    \"\"\"\n",
    "    if 'min' in filters:\n",
    "        key = f'{data_label}_min'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(min(data))\n",
    "\n",
    "    if 'max' in filters:\n",
    "        key = f'{data_label}_max'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(max(data))\n",
    "\n",
    "    if 'mean' in filters:\n",
    "        key = f'{data_label}_mean'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(mean(data))\n",
    "\n",
    "    if 'stdev' in filters:\n",
    "        key = f'{data_label}_stdev'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(stdev(data))\n",
    "\n",
    "    _quantiles = quantiles(data, n=4) #statistics.quantiles(data, n=4)\n",
    "\n",
    "    if 'q25' in filters:\n",
    "        key = f'{data_label}_q25'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(_quantiles[0])\n",
    "\n",
    "    if 'q50' in filters:\n",
    "        key = f'{data_label}_q50'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(_quantiles[1])\n",
    "\n",
    "    if 'q75' in filters:\n",
    "        key = f'{data_label}_q75'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(_quantiles[2])\n",
    "\n",
    "    if 'gini' in filters:\n",
    "        key = f'{data_label}_gini'\n",
    "        if key not in dictionary: dictionary[key]=[]\n",
    "        dictionary[key].append(gini(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e86ae9a7-5368-4bec-88c2-253b40ab6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_charactristics_info(graphs, build_representation_characteristics, \n",
    "                              out_path, out_file_name, weight_atts=None, \n",
    "                              append_to_file=False):\n",
    "    \"\"\"\n",
    "    param: a nx.graph list\n",
    "    param: build_representation_characteristics - function adds information based on representation (operations, heterogenous, disjunctive)\n",
    "    param: out_path - output directory\n",
    "    param: out_file_name - output file name\n",
    "    param: weight_atts -  edge attribus used to calculate centrality measures on graph\n",
    "    param: append_to_file - flag used to add information to existing graph charactistics files\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"out_file_name\", out_file_name,out_path)\n",
    "   \n",
    "    data_graph = build_graph_characteristics(graphs, weight_atts=weight_atts)\n",
    "    data_problem = build_representation_characteristics(graphs)\n",
    "   \n",
    "    data_graph.update(data_problem)\n",
    "\n",
    "    # for key, vals in data_graph.items():\n",
    "    #     print(key, len(vals))\n",
    "    \n",
    "    df = pd.DataFrame(data_graph)\n",
    "\n",
    "\n",
    "   \n",
    "    if append_to_file:\n",
    "        df.to_csv(f\"{out_path}/{out_file_name}\", sep=',', encoding='utf-8',\n",
    "        mode='a',  header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(f\"{out_path}/{out_file_name}\", sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a07a5ee-4fdb-4adf-98a6-86e7c561bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_graph_characteristics(graph, weight_att=None):\n",
    "    \"\"\"\n",
    "    param: graph - a nx.graph\n",
    "    param: weight_att - wheight of an edge (if None the weight is 1)\n",
    "    return: diameter, periphery nodes percentage, radius, center nodes percentage, assortativity\n",
    "    \"\"\"\n",
    "    no_nodes = graph.number_of_nodes()\n",
    "\n",
    "    diameter=nx.diameter(graph, weight=weight_att)\n",
    "    graph_node_periphery_percentage = len(list(nx.periphery(graph, weight=weight_att))) / no_nodes\n",
    "       \n",
    "    radius=nx.radius(graph, weight=weight_att)\n",
    "    graph_node_center_percentage = len(list(nx.center(graph, weight=weight_att))) / no_nodes\n",
    "\n",
    "    #Compute degree assortativity of graph.\n",
    "    #measure the similarity of connections in the graph with respect to the node degree\n",
    "    assortativity  = nx.degree_assortativity_coefficient(graph, weight=weight_att)\n",
    "\n",
    "    return diameter, graph_node_periphery_percentage, radius, graph_node_center_percentage, assortativity\n",
    "    \n",
    "def node_degree_centrality_measures(graph):\n",
    "    \"\"\"\n",
    "    param: graph - a nx.graph\n",
    "    return: \n",
    "    \"\"\"\n",
    "    # degree centrality that measures the visibility of a vertex among its neighbours (degree centrality, voterank (o varianta mai buna de degree centrality))\n",
    "    nodes_degree_centrality = [val for (_, val) in nx.degree_centrality(graph).items()]\n",
    "    return  nodes_degree_centrality\n",
    "   \n",
    "def node_centrality_measures(graph, weight_att=None):\n",
    "    \"\"\"\n",
    "    param: graph - a nx.graph\n",
    "    param: weight_att - wheight of an edge (if None the weight is 1)\n",
    "    return: path centrality that measures (betweenness_centrality, load_centrality);\n",
    "            proximity centrality (closeness_centrality); spectral centralities (eigvals)\n",
    "    \"\"\"\n",
    "    # path centrality that measures a vertex as being central if it is between of many \"path\" (betweenness centrality, load centrality) - utilizez masuri doar legate de shortest path, in unele cazuri random waks mai bune - random walks considers the contribution of all paths, not just the shortest ones, while placing more weight on the shortest paths and considering generating random walks \n",
    "    nodes_betweenness_centrality = nx.betweenness_centrality(graph, weight=weight_att )\n",
    "    nodes_load_centrality = nx.load_centrality(graph, weight=weight_att)\n",
    "    \n",
    "    # proximity centrality that  measures distance between a vertex and others (closeness centrality, Harmonic centrality (centrality measures give a more accurate measure of closeness in a case when some of the nodes are outside the perimeter of reach))\n",
    "    nodes_closeness_centrality = nx.closeness_centrality(graph, distance=weight_att)\n",
    "    \n",
    "    # spectral centralities: that measures  the vertices centrality by their participation in substructures of the network (eigenvector centrality)\n",
    "    L = nx.normalized_laplacian_matrix(graph, weight=weight_att)\n",
    "\n",
    "    nodes_eigenvalue = np.linalg.eigvals(L.toarray()).real#[:, np.newaxis].real\n",
    "\n",
    "    return nodes_betweenness_centrality, nodes_load_centrality, nodes_closeness_centrality, nodes_eigenvalue\n",
    "\n",
    "\n",
    "def edge_centraity_measures(graph, weight_att=None):\n",
    "    \"\"\"\n",
    "    param: graph - a nx.graph\n",
    "    param: weight_att - wheight of an edge (if None the weight is 1)\n",
    "    return: edge_betweenness_centrality\n",
    "    \"\"\"\n",
    "    # path centrality\n",
    "    edge_betweenness_centrality = [val for (_, val) in nx.edge_betweenness_centrality(graph, weight=weight_att).items()]\n",
    "    \n",
    "    return edge_betweenness_centrality\n",
    "\n",
    "def build_graph_characteristics(graphs, weight_atts):\n",
    "    \"\"\"\n",
    "    Extracts information using info stored in the graph\n",
    "    param: graph - a list of nx.graph to apply the nx library functions\n",
    "    param: weight_atts - a list with the values to store on edges\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    for g in graphs:\n",
    "        print(g.graph['name'])\n",
    "        \n",
    "        if 'id' not in data: data['id']=[]\n",
    "        data['id'].append(g.graph['name'])\n",
    "\n",
    "        #graph density\n",
    "        if 'graph_density' not in data: data['graph_density']=[]\n",
    "        data['graph_density'].append(nx.density(g))\n",
    "\n",
    "        #nodes degree \n",
    "        get_statistics([int(val) for (_, val) in g.degree()] , data, 'node_degree') \n",
    "\n",
    "        nodes_degree_centrality = node_degree_centrality_measures(g)\n",
    "        get_statistics(nodes_degree_centrality, data, f'nodes_degree_centrality')\n",
    "\n",
    "        for weight_att in weight_atts:\n",
    "            print(\"\\t\", weight_att)\n",
    "\n",
    "            str_weight_att = weight_att if weight_att is not None else 'unweighted'\n",
    "\n",
    "            #general_graph_characteristics\n",
    "            diameter, graph_node_periphery_percentage, radius, graph_node_center_percentage, assortativity = general_graph_characteristics(g, weight_att)\n",
    "    \n",
    "            key = f'graph_diameter_{str_weight_att}'\n",
    "            if key not in data: data[key]=[]\n",
    "            data[key].append(diameter)\n",
    "            \n",
    "            key = f'graph_node_periphery_percentage_{str_weight_att}'\n",
    "            if key not in data: data[key]=[]\n",
    "            data[key].append(graph_node_periphery_percentage)\n",
    "    \n",
    "            key = f'graph_radius_{str_weight_att}'\n",
    "            if key not in data: data[key]=[]\n",
    "            data[key].append(radius)\n",
    "    \n",
    "            key = f'graph_node_center_percentage_{str_weight_att}'\n",
    "            if key not in data: data[key]=[]\n",
    "            data[key].append(graph_node_center_percentage)\n",
    "    \n",
    "            key = f'graph_assortativity_{str_weight_att}'\n",
    "            if key not in data: data[key]=[]\n",
    "            data[key].append(assortativity)\n",
    "      \n",
    "            #node_centrality_measures\n",
    "            nodes_betweenness_centrality, nodes_load_centrality, nodes_closeness_centrality, nodes_eigenvalue = node_centrality_measures(g, weight_att)\n",
    "            get_statistics(list(nodes_betweenness_centrality.values()), data, f'nodes_betweenness_centrality_{str_weight_att}')\n",
    "            get_statistics(list(nodes_load_centrality.values()), data, f'nodes_load_centrality_{str_weight_att}')\n",
    "            get_statistics(list(nodes_closeness_centrality.values()), data, f'nodes_closeness_centrality_{str_weight_att}')\n",
    "            get_statistics(nodes_eigenvalue, data, f'nodes_eigenvalue_{str_weight_att}')\n",
    "\n",
    "            key = f'graph_second_eigenvalue_{str_weight_att}'\n",
    "            if key not in data: data[key]=[]\n",
    "            data[key].append(np.partition(list(nodes_closeness_centrality.values()), 2)[1])\n",
    "    \n",
    "            #edge_centraity_measures\n",
    "            edge_betweenness_centrality = edge_centraity_measures(g, weight_att)\n",
    "            get_statistics(edge_betweenness_centrality, data, f'edge_betweenness_centrality_{weight_att}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4364e-026a-40af-98e7-20dc5535bea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
